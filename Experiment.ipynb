{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89d2d112-8051-44d6-9352-d7639d76b646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb02403b-a3be-41bc-8ffe-5b5637c9015f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bceafbe5-23f9-4c73-b75c-7019d937ec01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-09 10:48:01.065248: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-09 10:48:01.074152: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741497481.085180   59529 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741497481.088905   59529 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-09 10:48:01.101331: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44cf1669-b585-4dd2-ac30-a3f68ec8f645",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data,train_label),(test_data,test_label) = fashion_mnist.load_data()\n",
    "class_names = [\"T-shirt\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \n",
    "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle_boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5666d7f-0569-4096-98b0-e25ed03930cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_data, train_label = sklearn.utils.shuffle(train_data, train_label)\n",
    "test_data, test_label = sklearn.utils.shuffle(test_data, test_label)\n",
    "\n",
    "train_data = train_data.reshape(train_data.shape[0],-1)\n",
    "test_data = test_data.reshape(test_data.shape[0],-1)\n",
    "train_data = train_data/255.0\n",
    "test_data = test_data/255.0\n",
    "train_label = np.eye(10)[train_label]\n",
    "test_label = np.eye(10)[test_label]\n",
    "\n",
    "train_data,val_data,train_label,val_label = train_test_split(train_data,train_label,test_size=0.1,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "647ad44d-684c-44e9-b5ea-e7d3c903c335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neural_net:\n",
    "    def __init__(self,num_hidden_layers,num_neurons_each_layer,activation_function,input_size,type_of_init,L2reg_const=0):\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_neurons_each_layer = num_neurons_each_layer\n",
    "        self.input_size = input_size\n",
    "        self.W,self.B = self.init_parameters(type_of_init)\n",
    "        self.activation_function = activation_function\n",
    "        self.L2reg_const = L2reg_const\n",
    "        \n",
    "\n",
    "    def init_parameters(self,init):\n",
    "        W = []\n",
    "        B = []\n",
    "        if init == 'xavier':\n",
    "            W.append(np.random.randn(self.num_neurons_each_layer[0], self.input_size) * np.sqrt(1 / self.input_size))\n",
    "            for i in range(1,self.num_hidden_layers):\n",
    "                W.append(np.random.randn(self.num_neurons_each_layer[i], self.num_neurons_each_layer[i-1]) * np.sqrt(1 / self.num_neurons_each_layer[i-1]))\n",
    "        else:\n",
    "            W.append(np.random.randn(self.num_neurons_each_layer[0], self.input_size) * 0.01)\n",
    "            for i in range(1, self.num_hidden_layers):\n",
    "                W.append(np.random.randn(self.num_neurons_each_layer[i], self.num_neurons_each_layer[i - 1]) * 0.01)\n",
    "\n",
    "        B.append(np.zeros(shape=(1,self.num_neurons_each_layer[0])))\n",
    "        for i in range(1,self.num_hidden_layers):\n",
    "            B.append(np.zeros(shape=(1,self.num_neurons_each_layer[i])))\n",
    "\n",
    "        return W, B\n",
    "\n",
    "    def activation(self,x):\n",
    "        if self.activation_function == 'ReLU':\n",
    "            return self.ReLU(x)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return self.tanh(x)\n",
    "        elif self.activation_function == 'sigmoid':\n",
    "            return self.sigmoid(x)\n",
    "\n",
    "    def activation_derivative(self,x):\n",
    "        if self.activation_function == 'ReLU':\n",
    "            return self.ReLU_derivative(x)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return self.tanh_derivative(x)\n",
    "        elif self.activation_function == 'sigmoid':\n",
    "            return self.sigmoid_derivative(x)\n",
    "\n",
    "    def tanh(self,x):\n",
    "        return np.array([((np.exp(z) - np.exp(-z))/((np.exp(z) + np.exp(-z)))) for z in x])\n",
    "\n",
    "    def tanh_derivative(self,x):\n",
    "        return np.array(1 - self.tanh(x)**2)\n",
    "    \n",
    "    def softmax_activation(self,x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    def ReLU(self,x):\n",
    "        return np.maximum(0,x)\n",
    "\n",
    "    def ReLU_derivative(self,x):\n",
    "        return np.where(x > 0, 1, 0)\n",
    "    \n",
    "    def sigmoid(self,x):\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1/(1+np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self,x):\n",
    "        return self.sigmoid(x)*(1-self.sigmoid(x))\n",
    "\n",
    "    def FeedForward(self,data):\n",
    "        a_l = []\n",
    "        h_l = []\n",
    "        input_linear_out = np.dot(data,self.W[0].T) + self.B[0]\n",
    "        a_l.append(input_linear_out)\n",
    "        for i in range(1,self.num_hidden_layers):\n",
    "            activation_out = self.activation(a_l[-1])\n",
    "            h_l.append(activation_out)\n",
    "            linear_out = np.dot(h_l[-1],self.W[i].T) + self.B[i]\n",
    "            a_l.append(linear_out)\n",
    "\n",
    "\n",
    "        y_hat = self.softmax_activation(a_l[-1])\n",
    "        return a_l,h_l,y_hat\n",
    "\n",
    "    def val_loss_and_acc(self,val_data,val_label):\n",
    "        acc = 0\n",
    "        error = 0\n",
    "        val_loss = []\n",
    "        val_acc = []\n",
    "        \n",
    "        for d in range(val_data.shape[0]):\n",
    "            a_li,h_li,y_hat_i = self.FeedForward(val_data[d].reshape(1,-1))\n",
    "            s = [x.sum() for x in self.W]\n",
    "            error += -np.sum(val_label[d] * np.log(y_hat_i[0])) + self.L2reg_const/2*sum(s)\n",
    "            if np.argmax(val_label[d]) == np.argmax(y_hat_i[0]):\n",
    "              acc += 1\n",
    "        return error/val_data.shape[0],acc/val_data.shape[0]\n",
    "\n",
    "    def backpropagation(self,A,H,y_hat,y,data):\n",
    "      dA = []\n",
    "      dH = []\n",
    "      dW = []\n",
    "      dB = []\n",
    "      dA.append(-(y - y_hat))\n",
    "      for i in range(self.num_hidden_layers-1,-1,-1):\n",
    "        if i == self.num_hidden_layers - 1:\n",
    "            grad_val_w = np.dot(dA[-1].reshape(-1, 1), H[-1].reshape(1, -1))\n",
    "        elif i == 0:\n",
    "          grad_val_w = np.dot(dA[-1].reshape(-1,1),data.reshape(1,-1))\n",
    "        else:\n",
    "          grad_val_w = np.dot(dA[-1].reshape(-1,1),H[i-1].reshape(1,-1))\n",
    "        dW.append(grad_val_w)\n",
    "        grad_val_b = np.sum(dA[-1],axis=0,keepdims=True)\n",
    "        dB.append(grad_val_b)\n",
    "\n",
    "        if i > 0:\n",
    "          grad_L_hi = np.dot(self.W[i].T,dA[-1].T)\n",
    "          grad_L_ai = np.multiply(grad_L_hi.T,self.activation_derivative(A[i-1]))\n",
    "          dA.append(grad_L_ai)\n",
    "      return dW[::-1],dB[::-1]\n",
    "\n",
    "    def train(self,optimizer,epochs,learning_rate,data,label,val_data,val_label,batch_size=32,**kwargs):\n",
    "        num_batches = len(data)//batch_size\n",
    "        optimizer_config = {'learning_rate':learning_rate,**kwargs}\n",
    "        if optimizer == 'momentum':\n",
    "            momentum_W = [np.zeros_like(w) for w in self.W]\n",
    "            momentum_B = [np.zeros_like(b) for b in self.B]\n",
    "            optimizer_config['momentum_W'] = momentum_W\n",
    "            optimizer_config['momentum_B'] = momentum_B\n",
    "        elif optimizer == 'nesterov':\n",
    "            momentum_W = [np.zeros_like(w) for w in self.W]\n",
    "            momentum_B = [np.zeros_like(b) for b in self.B]\n",
    "            optimizer_config['momentum_W'] = momentum_W\n",
    "            optimizer_config['momentum_B'] = momentum_B\n",
    "        elif optimizer == 'RMSprop':\n",
    "            v_W = [np.zeros_like(w) for w in self.W]\n",
    "            v_B = [np.zeros_like(b) for b in self.B]\n",
    "            optimizer_config['v_W'] = v_W\n",
    "            optimizer_config['v_B'] = v_B\n",
    "        elif optimizer == 'adam' or optimizer == \"nadam\":\n",
    "            momentum1_W = [np.zeros_like(w) for w in self.W]\n",
    "            momentum1_B = [np.zeros_like(b) for b in self.B]\n",
    "            momentum2_W = [np.zeros_like(w) for w in self.W]\n",
    "            momentum2_B = [np.zeros_like(b) for b in self.B]\n",
    "            optimizer_config['momentum1_W'] = momentum1_W\n",
    "            optimizer_config['momentum1_B'] = momentum1_B\n",
    "            optimizer_config['momentum2_W'] = momentum2_W\n",
    "            optimizer_config['momentum2_B'] = momentum2_B\n",
    "            optimizer_config['t'] = 0\n",
    "\n",
    "        for ep in range(epochs):\n",
    "            train_loss = 0\n",
    "            train_acc = 0\n",
    "            for batch in range(num_batches):\n",
    "                start = batch*batch_size\n",
    "                end = (batch+1)*batch_size\n",
    "                batch_data = data[start:end]\n",
    "                batch_label = label[start:end]\n",
    "                batch_dw = [np.zeros_like(w) for w in self.W]\n",
    "                batch_db = [np.zeros_like(b) for b in self.B]\n",
    "                batch_correct = 0\n",
    "                if optimizer in ('adam','nadam'):\n",
    "                    optimizer_config['t'] += 1\n",
    "\n",
    "                for i in range(batch_size):\n",
    "                    a_li,h_li,y_hat_i = self.FeedForward(batch_data[i].reshape(1,-1))\n",
    "                    s = [x.sum() for x in self.W]\n",
    "                    train_loss += -np.sum(batch_label[i] * np.log(y_hat_i[0])) + self.L2reg_const/2*sum(s)\n",
    "                    # train_loss += -np.sum(batch_label[i].reshape(1, -1) * np.log(y_hat_i[0].reshape(1, -1))) + self.L2reg_const/2*sum(s)\n",
    "\n",
    "                    if np.argmax(batch_label[i]) == np.argmax(y_hat_i[0]):\n",
    "                        batch_correct += 1\n",
    "                    dw, db = self.backpropagation(a_li, h_li, y_hat_i[0], batch_label[i], batch_data[i])\n",
    "                    for k in range(self.num_hidden_layers):\n",
    "                        batch_dw[k] += dw[k]\n",
    "                        batch_db[k] += db[k]\n",
    "                train_acc += batch_correct\n",
    "                self.W,self.B,optimizer_config = self.apply_optimizer(optimizer,batch_dw,batch_db,optimizer_config,batch_size)\n",
    "                if (batch + 1) % 10 == 0:\n",
    "                    val_loss, val_acc = self.val_loss_and_acc(val_data, val_label)\n",
    "                    wandb.log({'epoch': ep + 1, 'train_loss': train_loss / ((batch+1)*batch_size), 'train_acc': train_acc / ((batch+1)*batch_size), 'val_loss': val_loss, 'val_acc': val_acc})\n",
    "                    sys.stdout.write(f\"\\rEpoch {ep + 1}/{epochs} - Batch {batch + 1}/{num_batches} - Loss: {train_loss / ((batch+1)*batch_size):.6f} Train-Acc: {train_acc / ((batch+1)*batch_size):.6f} val-loss:{val_loss:.6f} val-Acc: {val_acc:.6f} \")\n",
    "                    sys.stdout.flush()\n",
    "            print()\n",
    "    def apply_optimizer(self,optimizer,batch_dw,batch_db,config,batch_size):\n",
    "        learning_rate = config['learning_rate']\n",
    "        if optimizer == 'sgd':\n",
    "            for k in range(self.num_hidden_layers):\n",
    "                self.W[k] -= (learning_rate/batch_size)*batch_dw[k]\n",
    "                self.B[k] -= (learning_rate/batch_size)*batch_db[k]\n",
    "        elif optimizer == 'momentum':\n",
    "            momentum_W = config['momentum_W']\n",
    "            momentum_B = config['momentum_B']\n",
    "            beta = config.get('beta',0.6)\n",
    "            for k in range(self.num_hidden_layers):\n",
    "                momentum_W[k] = beta*momentum_W[k] + batch_dw[k]\n",
    "                momentum_B[k] = beta*momentum_B[k] + batch_db[k]\n",
    "                self.W[k] -= learning_rate*momentum_W[k]\n",
    "                self.B[k] -= learning_rate*momentum_B[k]\n",
    "            config['momentum_W'] = momentum_W\n",
    "            config['momentum_B'] = momentum_B\n",
    "        elif optimizer == 'nesterov':\n",
    "            momentum_W = config['momentum_W']\n",
    "            momentum_B = config['momentum_B']\n",
    "            beta = config.get('beta',0.6)\n",
    "            for k in range(self.num_hidden_layers):\n",
    "                W_lookahead = [w - beta * v for w, v in zip(self.W, momentum_W)]\n",
    "                B_lookahead = [b - beta * v for b, v in zip(self.B, momentum_B)]\n",
    "                \n",
    "                momentum_W[k] = beta * momentum_W[k] + (learning_rate / batch_size) * batch_dw[k]\n",
    "                momentum_B[k] = beta * momentum_B[k] + (learning_rate / batch_size) * batch_db[k]\n",
    "                self.W[k] = W_lookahead[k] - momentum_W[k]\n",
    "                self.B[k] = B_lookahead[k] - momentum_B[k]\n",
    "\n",
    "            config['momentum_W'] = momentum_W\n",
    "            config['momentum_B'] = momentum_B\n",
    "\n",
    "        elif optimizer == 'RMSprop':\n",
    "            v_W = config['v_W']\n",
    "            v_B = config['v_B']\n",
    "            beta = config.get('beta', 0.9)\n",
    "            eps = config.get('eps', 1e-8)\n",
    "            for k in range(self.num_hidden_layers):\n",
    "                v_W[k] = beta * v_W[k] + (1 - beta) * (batch_dw[k]**2)\n",
    "                v_B[k] = beta * v_B[k] + (1 - beta) * (batch_db[k]**2)\n",
    "\n",
    "                adaptive_lr_w = (learning_rate / (np.sqrt(v_W[k]) + eps))\n",
    "                adaptive_lr_b = (learning_rate / (np.sqrt(v_B[k]) + eps))\n",
    "                self.W[k] -= adaptive_lr_w * batch_dw[k]\n",
    "                self.B[k] -= adaptive_lr_b * batch_db[k]\n",
    "\n",
    "            config['v_W'] = v_W\n",
    "            config['v_B'] = v_B\n",
    "        elif optimizer == 'adam':\n",
    "            momentum1_W = config['momentum1_W']\n",
    "            momentum1_B = config['momentum1_B']\n",
    "            momentum2_W = config['momentum2_W']\n",
    "            momentum2_B = config['momentum2_B']\n",
    "            t = config['t']\n",
    "            beta1 = config.get('beta1', 0.9)\n",
    "            beta2 = config.get('beta2', 0.999)\n",
    "            eps = config.get('eps', 1e-8)\n",
    "\n",
    "            for i in range(self.num_hidden_layers):\n",
    "                momentum1_W[i] = beta1*momentum1_W[i] + (1-beta1)*(batch_dw[i])\n",
    "                momentum1_B[i] = beta1*momentum1_B[i] + (1-beta1)*(batch_db[i])\n",
    "\n",
    "                momentum2_W[i] = beta2*momentum2_W[i] + (1-beta2)*(batch_dw[i]**2)\n",
    "                momentum2_B[i] = beta2*momentum2_B[i] + (1-beta2)*(batch_db[i]**2)\n",
    "\n",
    "                momentum1_W_hat = momentum1_W[i]/(1-(beta1**t))\n",
    "                momentum1_B_hat = momentum1_B[i]/(1-(beta1**t))\n",
    "\n",
    "                momentum2_W_hat = momentum2_W[i]/(1-(beta2**t))\n",
    "                momentum2_B_hat = momentum2_B[i]/(1-(beta2**t))\n",
    "\n",
    "                adaptive_lr_W = learning_rate/(np.sqrt(momentum2_W_hat) + eps)\n",
    "                adaptive_lr_B = learning_rate/(np.sqrt(momentum2_B_hat) + eps)\n",
    "\n",
    "                self.W[i] -= adaptive_lr_W * momentum1_W_hat\n",
    "                self.B[i] -= adaptive_lr_B * momentum1_B_hat\n",
    "            config['momentum1_W'] = momentum1_W\n",
    "            config['momentum1_B'] = momentum1_B\n",
    "            config['momentum2_W'] = momentum2_W\n",
    "            config['momentum2_B'] = momentum2_B\n",
    "            config['t'] = t\n",
    "        elif optimizer == 'nadam':\n",
    "            momentum1_W = config['momentum1_W']\n",
    "            momentum1_B = config['momentum1_B']\n",
    "            momentum2_W = config['momentum2_W']\n",
    "            momentum2_B = config['momentum2_B']\n",
    "            t = config['t']\n",
    "            beta1 = config.get('beta1', 0.9)\n",
    "            beta2 = config.get('beta2', 0.999)\n",
    "            eps = config.get('eps', 1e-8)\n",
    "\n",
    "            for i in range(self.num_hidden_layers):\n",
    "                momentum1_W[i] = beta1 * momentum1_W[i] + (1 - beta1) * (batch_dw[i])\n",
    "                momentum1_B[i] = beta1 * momentum1_B[i] + (1 - beta1) * (batch_db[i])\n",
    "\n",
    "                momentum2_W[i] = beta2 * momentum2_W[i] + (1 - beta2) * (batch_dw[i]**2)\n",
    "                momentum2_B[i] = beta2 * momentum2_B[i] + (1 - beta2) * (batch_db[i]**2)\n",
    "\n",
    "                momentum1_W_hat = momentum1_W[i] / (1 - (beta1**t))\n",
    "                momentum1_B_hat = momentum1_B[i] / (1 - (beta1**t))\n",
    "\n",
    "                momentum2_W_hat = momentum2_W[i] / (1 - (beta2**t))\n",
    "                momentum2_B_hat = momentum2_B[i] / (1 - (beta2**t))\n",
    "\n",
    "                m_nestrov_W = beta1 * momentum1_W_hat + ((1 - beta1) * batch_dw[i])/(1-beta1**t)\n",
    "                m_nestrov_B = beta1 * momentum1_B_hat + ((1 - beta1) * batch_db[i])/(1-beta1**t)\n",
    "\n",
    "                adaptive_lr_W = learning_rate / (np.sqrt(momentum2_W_hat) + eps)\n",
    "                adaptive_lr_B = learning_rate / (np.sqrt(momentum2_B_hat) + eps)\n",
    "\n",
    "                self.W[i] -= adaptive_lr_W * m_nestrov_W\n",
    "                self.B[i] -= adaptive_lr_B * m_nestrov_B\n",
    "            config['momentum1_W'] = momentum1_W\n",
    "            config['momentum1_B'] = momentum1_B\n",
    "            config['momentum2_W'] = momentum2_W\n",
    "            config['momentum2_B'] = momentum2_B\n",
    "            config['t'] = t\n",
    "\n",
    "        return self.W,self.B,config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a9dfca09-1b15-444b-913b-835ce67476e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Neural_net(num_hidden_layers=4, num_neurons_each_layer=[128, 64,32,10],activation_function='ReLU' ,input_size=784,type_of_init='xavier',L2reg_const=0.0005)\n",
    "optimizer = \"adam\"  # or \"sgd\", \"momentum\", \"nesterov\", \"RMSprop\", \"nadam\",'adam'\n",
    "learning_rate = 0.0001\n",
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "106f6725-5fa6-402b-a2c1-47daf5b890e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_kwargs = {}\n",
    "if optimizer == \"momentum\" or optimizer == \"nesterov\":\n",
    "    optimizer_kwargs['beta'] = 0.9\n",
    "elif optimizer == \"RMSprop\":\n",
    "    optimizer_kwargs['beta'] = 0.8\n",
    "    optimizer_kwargs['eps'] = 1e-8\n",
    "elif optimizer == \"adam\" or optimizer == \"nadam\":\n",
    "    optimizer_kwargs['beta1'] = 0.9\n",
    "    optimizer_kwargs['beta2'] = 0.999\n",
    "    optimizer_kwargs['eps'] = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1466fdf8-d7a7-4a3c-8216-172c6dff6fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'method': 'random',\n",
    "    'metric': {'goal': 'maximize', 'name': 'val_acc'},\n",
    "    'parameters': {\n",
    "        'activation_function': {'values': ['sigmoid', 'tanh', 'ReLU']},\n",
    "        'batch_size': {'values': [16, 32, 64]},\n",
    "        'epochs': {'values': [5, 10]},\n",
    "        'architecture': {\n",
    "            'values': [\n",
    "                {'num_of_hidden_layers': 3, 'hidden_layer_size': [128, 64, 10]},\n",
    "                {'num_of_hidden_layers': 4, 'hidden_layer_size': [256, 128, 64, 10]},\n",
    "                {'num_of_hidden_layers': 5, 'hidden_layer_size': [512, 256, 128, 64, 10]},\n",
    "                {'num_of_hidden_layers': 6, 'hidden_layer_size': [1024, 512, 256, 128, 64, 10]}\n",
    "            ]\n",
    "        },\n",
    "        'learning_rate': {'values': [1e-3,1e-4]},\n",
    "        'optimizer': {'values': ['sgd', 'momentum', 'nesterov', 'RMSprop', 'adam', 'nadam']},\n",
    "        'weight_initialization': {'values': ['random', 'xavier']},\n",
    "        'weight_decay': {'values': [0, 0.0005,0.5]},\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2eb2d2e9-c7f7-4b74-ab3b-e36e1b167e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    var1 = wandb.init()\n",
    "    config = var1.config\n",
    "\n",
    "    arch = config.architecture\n",
    "    # Set up the model based on sweep config\n",
    "    num_hidden_layers = arch['num_of_hidden_layers']\n",
    "    num_neurons_each_layer = arch['hidden_layer_size']\n",
    "    \n",
    "    nn = Neural_net(\n",
    "        num_hidden_layers=num_hidden_layers,\n",
    "        num_neurons_each_layer=num_neurons_each_layer,\n",
    "        activation_function = config.activation_function,\n",
    "        input_size=784,\n",
    "        type_of_init = config.weight_initialization,\n",
    "        L2reg_const = config.weight_decay\n",
    "    )\n",
    "    \n",
    "    nn.train(\n",
    "        optimizer=config.optimizer,\n",
    "        epochs=config.epochs,\n",
    "        learning_rate=config.learning_rate,\n",
    "        data=train_data,  # Make sure to define train_data and labels\n",
    "        label=train_label,\n",
    "        val_data=val_data,\n",
    "        val_label=val_label,\n",
    "        batch_size=config.batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c7ebd369-4c43-4513-a4be-3a6bd9f8c9d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcs24s031\u001b[0m (\u001b[33mcs24s031-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a7c31bda-c776-487f-8bf1-993afe825b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: lyox2jab\n",
      "Sweep URL: https://wandb.ai/cs24s031-indian-institute-of-technology-madras/dl-assignment1-experiment/sweeps/lyox2jab\n"
     ]
    }
   ],
   "source": [
    "sweep_id = wandb.sweep(sweep_config,project='dl-assignment1-experiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8cac592d-1fc6-4522-901c-837ee858dc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: c4pw1x1l with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation_function: sigmoid\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tarchitecture: {'hidden_layer_size': [512, 256, 128, 64, 10], 'num_of_hidden_layers': 5}\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 5\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.0003\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: nesterov\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_initialization: random\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "creating run (0.0s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dextrolaev/Desktop/DL/da6401_assignment1/wandb/run-20250309_111457-c4pw1x1l</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/cs24s031-indian-institute-of-technology-madras/dl-assignment1-experiment/runs/c4pw1x1l' target=\"_blank\">crisp-sweep-1</a></strong> to <a href='https://wandb.ai/cs24s031-indian-institute-of-technology-madras/dl-assignment1-experiment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/cs24s031-indian-institute-of-technology-madras/dl-assignment1-experiment/sweeps/lyox2jab' target=\"_blank\">https://wandb.ai/cs24s031-indian-institute-of-technology-madras/dl-assignment1-experiment/sweeps/lyox2jab</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/cs24s031-indian-institute-of-technology-madras/dl-assignment1-experiment' target=\"_blank\">https://wandb.ai/cs24s031-indian-institute-of-technology-madras/dl-assignment1-experiment</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/cs24s031-indian-institute-of-technology-madras/dl-assignment1-experiment/sweeps/lyox2jab' target=\"_blank\">https://wandb.ai/cs24s031-indian-institute-of-technology-madras/dl-assignment1-experiment/sweeps/lyox2jab</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/cs24s031-indian-institute-of-technology-madras/dl-assignment1-experiment/runs/c4pw1x1l' target=\"_blank\">https://wandb.ai/cs24s031-indian-institute-of-technology-madras/dl-assignment1-experiment/runs/c4pw1x1l</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Batch 90/3375 - Loss: 2.303133 Train-Acc: 0.108333 val-loss:2.302826 val-Acc: 0.103667 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Ctrl + C detected. Stopping sweep.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Batch 120/3375 - Loss: 2.303806 Train-Acc: 0.106771 val-loss:2.302791 val-Acc: 0.103667 "
     ]
    }
   ],
   "source": [
    "wandb.agent(sweep_id,train,count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059ada3b-c0a5-452f-a06e-01e485a280e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
